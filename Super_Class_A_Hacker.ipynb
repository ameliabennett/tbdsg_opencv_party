{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8be548265f75dd570e960bd1c29f60e6 (1).jpg',\n",
       " '8be548265f75dd570e960bd1c29f60e6.jpg',\n",
       " 'deploy (1).prototxt',\n",
       " 'deploy.prototxt',\n",
       " 'image (1).png',\n",
       " 'image.png',\n",
       " 'object_tracker (1).py',\n",
       " 'object_tracker.py',\n",
       " 'OpenCV_CharPy (1).pptx',\n",
       " 'OpenCV_CharPy.pptx',\n",
       " 'Open_CV_Talk_BAC_Edition.ipynb',\n",
       " 'Paint With OpenCV (1).ipynb',\n",
       " 'Paint With OpenCV.ipynb',\n",
       " 'pyimagesearch',\n",
       " 'res10_300x300_ssd_iter_140000 (1).caffemodel',\n",
       " 'res10_300x300_ssd_iter_140000.caffemodel',\n",
       " 'simple-object-tracking',\n",
       " 'smiley.png',\n",
       " 'Super_Class_A_Hacker (1).ipynb',\n",
       " 'Super_Class_A_Hacker.ipynb',\n",
       " 'Synthetic Shapes (1).ipynb',\n",
       " 'Synthetic Shapes.ipynb',\n",
       " 'SyntheticShapes (1).py',\n",
       " 'SyntheticShapes.py',\n",
       " 'yet_another_smiley.jpg',\n",
       " '~$OpenCV_CharPy (1).pptx',\n",
       " '~$OpenCV_CharPy.pptx']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "glob.glob(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "opencv_img = cv2.imread('image.png')\n",
    "cv2.imshow('testing',opencv_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "gits_image = cv2.imread('8be548265f75dd570e960bd1c29f60e6.jpg',1)\n",
    "#gits_image = cv2.imread('yet_another_smiley.jpg',1)\n",
    "cv2.imshow('laughing_man',gits_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gits_gray = cv2.cvtColor(gits_image, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('laughing_man',gits_gray)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "th, im_th = cv2.threshold(gits_gray, 220, 255, cv2.THRESH_BINARY_INV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('laughing_man',im_th)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_floodfill = im_th.copy()\n",
    "im_floodfill = im_th.copy()\n",
    " \n",
    "# Mask used to flood filling.\n",
    "# Notice the size needs to be 2 pixels than the image.\n",
    "h, w = im_th.shape[:2]\n",
    "mask = np.zeros((h+2, w+2), np.uint8)\n",
    " \n",
    "# Floodfill from point (0, 0)\n",
    "cv2.floodFill(im_floodfill, mask, (0,0), 255);\n",
    "cv2.imshow(\"Floodfilled Image\", im_floodfill)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert floodfilled image\n",
    "im_floodfill_inv = cv2.bitwise_not(im_floodfill)\n",
    " \n",
    "# Combine the two images to get the foreground.\n",
    "im_out = im_th | im_floodfill_inv\n",
    "cv2.imshow(\"Floodfilled Image\", im_out)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gits_gray = cv2.cvtColor(gits_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "th, im_th = cv2.threshold(gits_gray, 220, 255, cv2.THRESH_BINARY_INV);\n",
    " \n",
    "# Copy the thresholded image.\n",
    "im_floodfill = im_th.copy()\n",
    " \n",
    "# Mask used to flood filling.\n",
    "# Notice the size needs to be 2 pixels than the image.\n",
    "h, w = im_th.shape[:2]\n",
    "mask = np.zeros((h+2, w+2), np.uint8)\n",
    " \n",
    "# Floodfill from point (0, 0)\n",
    "cv2.floodFill(im_floodfill, mask, (0,0), 255);\n",
    " \n",
    "# Invert floodfilled image\n",
    "im_floodfill_inv = cv2.bitwise_not(im_floodfill)\n",
    " \n",
    "# Combine the two images to get the foreground.\n",
    "im_out = im_th | im_floodfill_inv\n",
    " \n",
    "# Display images.\n",
    "cv2.imshow(\"Thresholded Image\", im_th)\n",
    "cv2.imshow(\"Floodfilled Image\", im_floodfill)\n",
    "cv2.imshow(\"Inverted Floodfilled Image\", im_floodfill_inv)\n",
    "cv2.imshow(\"Foreground\", im_out)\n",
    "cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('laughing_man',gits_gray)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshhold_gits = cv2.bitwise_and(gits_image,gits_image, im_th)\n",
    "\n",
    "cv2.imshow(\"testing\", threshhold_gits)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 1200, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshhold_gits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imshow(\"Foreground\", im_out)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "#fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "#out = cv2.VideoWriter('output.avi',fourcc, 20.0, (640,480))\n",
    "\n",
    "# C:\\opencv\\opencv-master\\data\\haarcascades\\haarcascade_frontalcatface.xml\n",
    "\n",
    "#face_cascade = cv2.CascadeClassifier('C:\\opencv\\opencv-master\\data\\haarcascades\\haarcascade_frontalcatface.xml')\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret==True:\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        for (x,y,w,h) in faces:\n",
    "            cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "            #roi_gray = gray[y:y+h, x:x+w]\n",
    "            #roi_color = img[y:y+h, x:x+w]\n",
    "        #frame = cv2.flip(frame,0)\n",
    "\n",
    "        # write the flipped frame\n",
    "        #out.write(frame)\n",
    "\n",
    "        cv2.imshow('frame',frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release everything if job is finished\n",
    "cap.release()\n",
    "#out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyimagesearch.centroidtracker import CentroidTracker\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "ct = CentroidTracker()\n",
    "\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "#fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "#out = cv2.VideoWriter('output.avi',fourcc, 20.0, (640,480))\n",
    "\n",
    "# C:\\opencv\\opencv-master\\data\\haarcascades\\haarcascade_frontalcatface.xml\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('C:\\opencv\\opencv-master\\data\\haarcascades\\haarcascade_frontalcatface.xml')\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "rects = []\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret==True:\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        for (x,y,w,h) in faces:\n",
    "            cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "            boxxy = (x,y,x+w,y+h)\n",
    "            rects.append(boxxy)\n",
    "            #roi_gray = gray[y:y+h, x:x+w]\n",
    "            #roi_color = img[y:y+h, x:x+w]\n",
    "        #frame = cv2.flip(frame,0)\n",
    "        objects = ct.update(rects)\n",
    "        # write the flipped frame\n",
    "        #out.write(frame)\n",
    "        for (objectID, centroid) in objects.items():\n",
    "            text = \"ID {}\".format(objectID)\n",
    "            cv2.putText(frame, text, (centroid[0] - 10, centroid[1] - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            cv2.circle(frame, (centroid[0], centroid[1]), 4, (0, 255, 0), -1)\n",
    "        \n",
    "        cv2.imshow('frame',frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release everything if job is finished\n",
    "cap.release()\n",
    "#out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyimagesearch.centroidtracker import CentroidTracker\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "ct = CentroidTracker()\n",
    "\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "#fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "#out = cv2.VideoWriter('output.avi',fourcc, 20.0, (640,480))\n",
    "\n",
    "# C:\\opencv\\opencv-master\\data\\haarcascades\\haarcascade_frontalcatface.xml\n",
    "\n",
    "net = cv2.dnn.readNetFromCaffe('deploy.prototxt', 'res10_300x300_ssd_iter_140000.caffemodel')\n",
    "rects = []\n",
    "(H, W) = (None, None)\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret==True:\n",
    "        #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        #faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        if W is None or H is None:\n",
    "            (H, W) = frame.shape[:2]\n",
    "        \n",
    "        blob = cv2.dnn.blobFromImage(frame, 1.0, (W, H),(104.0, 177.0, 123.0))\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "        \n",
    "        for i in range(0, detections.shape[2]):\n",
    "\t\t# filter out weak detections by ensuring the predicted\n",
    "\t\t# probability is greater than a minimum threshold\n",
    "            if detections[0, 0, i, 2] > 0.95:\n",
    "                # compute the (x, y)-coordinates of the bounding box for\n",
    "                # the object, then update the bounding box rectangles list\n",
    "                box = detections[0, 0, i, 3:7] * np.array([W, H, W, H])\n",
    "                rects.append(box.astype(\"int\"))\n",
    "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "                cv2.rectangle(frame, (startX, startY), (endX, endY),(0, 255, 0), 2)\n",
    "        \n",
    "        #for (x,y,w,h) in faces:\n",
    "        #    cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        #    boxxy = (x,y,x+w,y+h)\n",
    "        #    rects.append(boxxy)\n",
    "            #roi_gray = gray[y:y+h, x:x+w]\n",
    "            #roi_color = img[y:y+h, x:x+w]\n",
    "        #frame = cv2.flip(frame,0)\n",
    "        objects = ct.update(rects)\n",
    "        # write the flipped frame\n",
    "        #out.write(frame)\n",
    "        for (objectID, centroid) in objects.items():\n",
    "            #text = \"ID {}\".format(objectID)\n",
    "            #cv2.putText(frame, text, (centroid[0] - 10, centroid[1] - 10),\n",
    "            #cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            cv2.circle(frame, (centroid[0], centroid[1]), 4, (0, 255, 0), -1)\n",
    "        \n",
    "        cv2.imshow('frame',frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release everything if job is finished\n",
    "cap.release()\n",
    "#out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyimagesearch.centroidtracker import CentroidTracker\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "ct = CentroidTracker()\n",
    "\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "#fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "#out = cv2.VideoWriter('output.avi',fourcc, 20.0, (640,480))\n",
    "\n",
    "# C:\\opencv\\opencv-master\\data\\haarcascades\\haarcascade_frontalcatface.xml\n",
    "\n",
    "net = cv2.dnn.readNetFromCaffe('deploy.prototxt', 'res10_300x300_ssd_iter_140000.caffemodel')\n",
    "rects = []\n",
    "(H, W) = (None, None)\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret==True:\n",
    "        #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        #faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        if W is None or H is None:\n",
    "            (H, W) = frame.shape[:2]\n",
    "        \n",
    "        blob = cv2.dnn.blobFromImage(frame, 1.0, (W, H),(104.0, 177.0, 123.0))\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "        \n",
    "        for i in range(0, detections.shape[2]):\n",
    "\t\t# filter out weak detections by ensuring the predicted\n",
    "\t\t# probability is greater than a minimum threshold\n",
    "            if detections[0, 0, i, 2] > 0.95:\n",
    "                # compute the (x, y)-coordinates of the bounding box for\n",
    "                # the object, then update the bounding box rectangles list\n",
    "                box = detections[0, 0, i, 3:7] * np.array([W, H, W, H])\n",
    "                #rects.append(box.astype(\"int\"))\n",
    "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "                x_delta = endX - startX\n",
    "                y_delta = endY - startY\n",
    "                greatest_delta = max(x_delta,y_delta)\n",
    "                resized = cv2.resize(threshhold_gits,(greatest_delta, greatest_delta), interpolation = cv2.INTER_CUBIC)\n",
    "                resized_imout = cv2.resize(im_out,(greatest_delta, greatest_delta), interpolation = cv2.INTER_CUBIC)\n",
    "                cv2.rectangle(frame, (startX, startY), (endX, endY),(0, 255, 0), 2)\n",
    "                try:\n",
    "                    frame[startY:startY+greatest_delta, startX:startX+greatest_delta] = resized\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        cv2.imshow('frame',frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release everything if job is finished\n",
    "cap.release()\n",
    "#out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyimagesearch.centroidtracker import CentroidTracker\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "ct = CentroidTracker()\n",
    "\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "#fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "#out = cv2.VideoWriter('output.avi',fourcc, 20.0, (640,480))\n",
    "\n",
    "# C:\\opencv\\opencv-master\\data\\haarcascades\\haarcascade_frontalcatface.xml\n",
    "\n",
    "net = cv2.dnn.readNetFromCaffe('deploy.prototxt', 'res10_300x300_ssd_iter_140000.caffemodel')\n",
    "rects = []\n",
    "(H, W) = (None, None)\n",
    "\n",
    "def transparentOverlay(src , overlay , mask, pos=(0,0),scale = 1):\n",
    "    \"\"\"\n",
    "    :param src: Input Color Background Image\n",
    "    :param overlay: transparent Image (BGRA)\n",
    "    :param pos:  position where the image to be blit.\n",
    "    :param scale : scale factor of transparent image.\n",
    "    :return: Resultant Image\n",
    "    \"\"\"\n",
    "    overlay = cv2.resize(overlay,(0,0),fx=scale,fy=scale)\n",
    "    h,w,_ = overlay.shape  # Size of foreground\n",
    "    rows,cols,_ = src.shape  # Size of background Image\n",
    "    y,x = pos[0],pos[1]    # Position of foreground/overlay image\n",
    "    \n",
    "    #loop over all pixels and apply the blending equation\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            if x+i >= rows or y+j >= cols:\n",
    "                continue\n",
    "            if mask[x+i][y+j] == 0:\n",
    "                src[x+i][y+j] = src[x+i][y+j]\n",
    "            else:\n",
    "                src[x+i][y+j] = overlay[x+i][y+j]\n",
    "    return src\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret==True:\n",
    "        #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        #faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        if W is None or H is None:\n",
    "            (H, W) = frame.shape[:2]\n",
    "        \n",
    "        blob = cv2.dnn.blobFromImage(frame, 1.0, (W, H),(104.0, 177.0, 123.0))\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "        \n",
    "        for i in range(0, detections.shape[2]):\n",
    "\t\t# filter out weak detections by ensuring the predicted\n",
    "\t\t# probability is greater than a minimum threshold\n",
    "            if detections[0, 0, i, 2] > 0.95:\n",
    "                # compute the (x, y)-coordinates of the bounding box for\n",
    "                # the object, then update the bounding box rectangles list\n",
    "                box = detections[0, 0, i, 3:7] * np.array([W, H, W, H])\n",
    "                #rects.append(box.astype(\"int\"))\n",
    "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "                x_delta = endX - startX\n",
    "                y_delta = endY - startY\n",
    "                greatest_delta = max(x_delta,y_delta)\n",
    "                resized = cv2.resize(threshhold_gits,(greatest_delta, greatest_delta), interpolation = cv2.INTER_CUBIC)\n",
    "                resized_imout = cv2.resize(im_out,(greatest_delta, greatest_delta), interpolation = cv2.INTER_CUBIC)\n",
    "                #cv2.rectangle(frame, (startX, startY), (endX, endY),(0, 255, 0), 2)\n",
    "                frame[startY:startY+greatest_delta, startX:startX+greatest_delta] = transparentOverlay(frame[startY:startY+greatest_delta, startX:startX+greatest_delta],\n",
    "                                                                                                       resized, resized_imout,\n",
    "                                                                                                       pos=(0,0),scale = 1)\n",
    "        cv2.imshow('frame',frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release everything if job is finished\n",
    "cap.release()\n",
    "#out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
